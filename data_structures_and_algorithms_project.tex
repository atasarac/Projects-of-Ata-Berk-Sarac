\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry} % Adjusted margins to fit content nicely on ~5 pages
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{float}

% Java styling
\lstdefinestyle{myjava}{
  language=Java,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{olive}\itshape,
  stringstyle=\color{red},
  frame=single,
  breaklines=true,
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny\color{gray},
  captionpos=b,
  tabsize=4
}

\title{Comparative Analysis of Sorting and Search Algorithms}
\author{Ata Berk Sara√ß}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report provides a comparative performance analysis of fundamental sorting and search algorithms. We examine the runtime behavior of Merge Sort versus Quick Sort under varying data permutations, highlighting the impact of pivot selection. Additionally, we contrast the lookup efficiency of Binary Search Trees (BST) against Hash Tables, specifically demonstrating the degradation of unbalanced BSTs in worst-case scenarios. All implementations are provided in Java.
\end{abstract}

\section{Sorting Algorithms}

\subsection{Overview}
We compare \textbf{Merge Sort} and \textbf{Quick Sort}. While both are divide-and-conquer algorithms, they differ fundamentally in memory usage and stability.

\begin{table}[h]
    \centering
    \caption{Time and Space Complexity Comparison}
    \vspace{0.1cm}
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Algorithm} & \textbf{Best Case} & \textbf{Average Case} & \textbf{Worst Case} \\ \midrule
        Merge Sort         & $\mathcal{O}(n \log n)$ & $\mathcal{O}(n \log n)$ & $\mathcal{O}(n \log n)$ \\
        Quick Sort         & $\mathcal{O}(n \log n)$ & $\mathcal{O}(n \log n)$ & $\mathcal{O}(n^2)$ \\ \bottomrule
    \end{tabular}
    \label{tab:complexity}
\end{table}

\noindent
\textbf{Key Distinctions:}
\begin{itemize}
    \item \textbf{Merge Sort}: Stable sort. Requires $\mathcal{O}(n)$ auxiliary space for merging.
    \item \textbf{Quick Sort}: Unstable sort. Functions in-place (requiring only $\mathcal{O}(\log n)$ stack space) but suffers quadratic degradation if the pivot partitions the array poorly (e.g., 0 elements vs $n-1$).
\end{itemize}

\subsection{Java Implementation & Experiment}

The Java implementation below includes a manual implementation of Merge Sort and Quick Sort (Lomuto partition scheme). The `main` method measures execution time in nanoseconds for higher precision.

\begin{lstlisting}[style=myjava, caption={Sorting Algorithms Implementation}]
import java.util.Arrays;
import java.util.Random;

public class SortingComparison {

    // --- Merge Sort Implementation ---
    public static void mergeSort(int[] arr, int left, int right) {
        if (left < right) {
            int mid = left + (right - left) / 2;
            mergeSort(arr, left, mid);
            mergeSort(arr, mid + 1, right);
            merge(arr, left, mid, right);
        }
    }

    private static void merge(int[] arr, int left, int mid, int right) {
        int n1 = mid - left + 1;
        int n2 = right - mid;
        int[] L = new int[n1];
        int[] R = new int[n2];

        for (int i = 0; i < n1; ++i) L[i] = arr[left + i];
        for (int j = 0; j < n2; ++j) R[j] = arr[mid + 1 + j];

        int i = 0, j = 0, k = left;
        while (i < n1 && j < n2) {
            if (L[i] <= R[j]) arr[k++] = L[i++];
            else              arr[k++] = R[j++];
        }
        while (i < n1) arr[k++] = L[i++];
        while (j < n2) arr[k++] = R[j++];
    }

    // --- Quick Sort Implementation (Lomuto Partition) ---
    public static void quickSort(int[] arr, int low, int high) {
        if (low < high) {
            int pi = partition(arr, low, high);
            quickSort(arr, low, pi - 1);
            quickSort(arr, pi + 1, high);
        }
    }

    private static int partition(int[] arr, int low, int high) {
        int pivot = arr[high];
        int i = (low - 1);
        for (int j = low; j < high; j++) {
            if (arr[j] < pivot) {
                i++;
                int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp;
            }
        }
        int temp = arr[i + 1]; arr[i + 1] = arr[high]; arr[high] = temp;
        return i + 1;
    }

    public static void main(String[] args) {
        int N = 2000; // Dataset size
        System.out.println("Sorting Performance Analysis (N=" + N + ")");
        
        // Case 1: Sorted Data (Worst Case for QuickSort)
        int[] sorted = new int[N];
        for(int i=0; i<N; i++) sorted[i] = i;

        int[] m1 = sorted.clone();
        long start = System.nanoTime();
        mergeSort(m1, 0, N-1);
        double tMergeSorted = (System.nanoTime() - start) / 1e6;

        int[] q1 = sorted.clone();
        start = System.nanoTime();
        quickSort(q1, 0, N-1);
        double tQuickSorted = (System.nanoTime() - start) / 1e6;

        System.out.printf("Sorted Input -> Merge: %.3f ms | Quick: %.3f ms\n", 
                          tMergeSorted, tQuickSorted);

        // Case 2: Random Data
        int[] random = new int[N];
        Random rand = new Random();
        for(int i=0; i<N; i++) random[i] = rand.nextInt(100000);

        int[] m2 = random.clone();
        start = System.nanoTime();
        mergeSort(m2, 0, N-1);
        double tMergeRandom = (System.nanoTime() - start) / 1e6;

        int[] q2 = random.clone();
        start = System.nanoTime();
        quickSort(q2, 0, N-1);
        double tQuickRandom = (System.nanoTime() - start) / 1e6;

        System.out.printf("Random Input -> Merge: %.3f ms | Quick: %.3f ms\n", 
                          tMergeRandom, tQuickRandom);
    }
}
\end{lstlisting}

\subsection{Results Discussion}
Running the Java simulation with $N=2000$ yields distinct performance profiles:
\begin{itemize}
    \item \textbf{Sorted Input}: Quick Sort is significantly slower. The Lomuto partition scheme picks the last element as the pivot; if the array is already sorted, this results in the most unbalanced partition possible, driving complexity to $\mathcal{O}(n^2)$. Merge Sort remains stable at $\mathcal{O}(n \log n)$.
    \item \textbf{Random Input}: Quick Sort typically outperforms Merge Sort. Despite the same asymptotic complexity, Quick Sort benefits from better cache locality and lacks the overhead of array copying required by Merge Sort's auxiliary storage.
\end{itemize}

\newpage

\section{Search Algorithms}

\subsection{Data Structure Comparison}
We compare the lookup efficiency of two distinct structures:
\begin{enumerate}
    \item \textbf{Binary Search Tree (BST)}: A hierarchical structure. If data is inserted in random order, height is $\mathcal{O}(\log n)$. However, inserting sorted data (e.g., $1, 2, 3...$) creates a "skewed" tree with height $\mathcal{O}(n)$, degenerating into a linked list.
    \item \textbf{Hash Table}: Uses a hash function to map keys to buckets. We implement a "chaining" strategy using linked lists to handle collisions. With a good hash function and load factor, lookup is $\mathcal{O}(1)$.
\end{enumerate}

\subsection{Java Implementation}
We provide a full implementation of a BST and a Chained Hash Table, alongside a driver to test Worst-Case insertion scenarios.

\begin{lstlisting}[style=myjava, caption={Search Structures and Benchmark}]
import java.util.LinkedList;

// --- 1. Binary Search Tree Implementation ---
class BST {
    class Node {
        int key;
        Node left, right;
        public Node(int item) { key = item; left = right = null; }
    }
    Node root;

    void insert(int key) { root = insertRec(root, key); }

    Node insertRec(Node root, int key) {
        if (root == null) { return new Node(key); }
        if (key < root.key) root.left = insertRec(root.left, key);
        else if (key > root.key) root.right = insertRec(root.right, key);
        return root;
    }

    boolean search(int key) { return searchRec(root, key); }

    boolean searchRec(Node root, int key) {
        if (root == null) return false;
        if (root.key == key) return true;
        return key < root.key ? searchRec(root.left, key) : searchRec(root.right, key);
    }
}

// --- 2. Hash Table Implementation (Chaining) ---
class HashTable {
    private int size;
    private LinkedList<Integer>[] table;

    @SuppressWarnings("unchecked")
    public HashTable(int size) {
        this.size = size;
        table = new LinkedList[size];
        for (int i = 0; i < size; i++) table[i] = new LinkedList<>();
    }

    private int hash(int key) {
        return Math.abs(key) % size;
    }

    public void put(int key) {
        int idx = hash(key);
        if (!table[idx].contains(key)) {
            table[idx].add(key);
        }
    }

    public boolean get(int key) {
        int idx = hash(key);
        return table[idx].contains(key);
    }
}

// --- 3. Benchmark Driver ---
public class SearchComparison {
    public static void main(String[] args) {
        int N = 2000;
        BST bst = new BST();
        HashTable ht = new HashTable(97); // Prime size for better distribution

        // 1. Worst Case Insertion: Sorted Data
        // This causes the BST to become a Linked List (depth 2000)
        System.out.println("Inserting " + N + " sorted integers...");
        for (int i = 0; i < N; i++) {
            bst.insert(i);
            ht.put(i);
        }

        // 2. Worst Case Search: Searching for the last element (N-1)
        System.out.println("Benchmarking search for key: " + (N-1));

        long start = System.nanoTime();
        boolean bstFound = bst.search(N - 1);
        long bstTime = System.nanoTime() - start;

        start = System.nanoTime();
        boolean htFound = ht.get(N - 1);
        long htTime = System.nanoTime() - start;

        System.out.printf("BST Search Time:       %d ns\n", bstTime);
        System.out.printf("Hash Table Search Time: %d ns\n", htTime);
    }
}
\end{lstlisting}

\subsection{Performance Analysis}

\subsubsection{BST Degradation}
In our experiment, we inserted keys $0$ through $1999$ sequentially.
\begin{itemize}
    \item Because the input was sorted, the BST did not branch. Every new node was added as the right child of the previous one.
    \item \textbf{Depth}: The tree depth reached $N=2000$.
    \item \textbf{Search Consequence}: Searching for the last inserted element ($1999$) required traversing 2000 nodes, resulting in $\mathcal{O}(N)$ performance, drastically slower than the theoretical $\mathcal{O}(\log n)$ average.
\end{itemize}

\subsubsection{Hash Table Resilience}
The Hash Table (size $M=97$) handled the sequential data differently:
\begin{itemize}
    \item Keys were distributed across 97 buckets using the modulo operator.
    \item Even with collisions (chaining), the average list length was approx $N/M \approx 20$.
    \item \textbf{Result}: The lookup required traversing a short linked list, maintaining near $\mathcal{O}(1)$ performance relative to $N$.
\end{itemize}

\section{Conclusion}
The choice of algorithm must be dictated by the data characteristics:
\begin{enumerate}
    \item For \textbf{Sorting}, Merge Sort provides safety guarantees for worst-case inputs, whereas Quick Sort offers speed for randomized data.
    \item For \textbf{Searching}, a standard BST is risky if the input might be pre-sorted. In such cases, a Hash Table or a self-balancing tree (like an AVL or Red-Black tree) is strictly superior.
\end{enumerate}

\end{document}
